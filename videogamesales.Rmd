---
title: "Developing and Evaluating a Video Game Rating Model"
author: "Kevin Camp"
date: "`r Sys.Date()`"
output: pdf_document
header-includes:
    - \usepackage{caption}
---

```{r setup, include=FALSE}
## Set knitr options
knitr::opts_chunk$set(echo = TRUE)
```

# 1.0 Introduction  

This report satisfies the guidelines for the Choose Your Own project in the Data Science Capstone course. My project goal is to analyzing a dataset of video game ratings by users in order to create a recommendation system. The recommendation system will predict ratings by user for games that are missing user ratings in the dataset. The dataset is large; that presents a challenge, in that large datasets are difficult to manage. It also presents an opportunity, in that every data point can be used as a predictor for each missing value in the overall set. The aim of the project is to achieve that using machine learning methods.

# 1.1 Overview  

The project uses data from this [repository](https://www.kaggle.com/datasets/rush4ratio/video-game-sales-with-ratings?resource=download) split into two sets. One set---"vg"---is used to train the model, and contains more than 15 thousand rows of video games. Each row provides columns detailing the name of the game (Name); the platform on which the game was released (Platform); the year of release (Year_of_Release); publisher (Publisher); five columns of sales data, including North America, European Union, Japan, Other, and Global; critic ratings (Critic_Score); critic rating count (Critic_Count); user ratings (User_Score); user rating counts (User_Count); developer (Developer); and rating (Rating). The other set---"validation"---is similar in content to vg, and is used as the final hold-out test set to see how well the model performs. Table 1.1 presents a selection of rows from vg.  

\captionsetup[table]{labelformat=empty}

```{r create datasets and view vg top, message=FALSE, warning=FALSE, echo=FALSE}
## Create vg set, validation set (final hold-out test set), set more options, install packages

options(repos = list(CRAN="http://cran.rstudio.com/"))

knitr::opts_chunk$set(message = FALSE)

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(psych)) install.packages("psych", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(jtools)) install.packages("jtools", repos = "http://cran.us.r-project.org")
if(!require(huxtable)) install.packages("huxtable", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)
library(kableExtra)
library(psych)
library(gridExtra)
library(jtools)
library(huxtable)

## Original data from kaggle.com user rush4ratio
## Data includes video game sales from Vgchartz and ratings from Metacritic
## Data now hosted on my dropbox

download.file("https://www.dropbox.com/s/pkkuwg8et5qdj70/Video_Games_Sales_as_at_22_Dec_2016.csv?dl=1","vg.csv", mode = "wb")

data = read.csv("vg.csv")

## Validation set will be 10% of vg data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = data$User_Score, times = 1, p = 0.1, list = FALSE)
vg <- data[-test_index,]
temp <- data[test_index,]

## Make sure Name and Platform in validation set are also in vgs set
validation <- temp %>% 
  semi_join(vg, by = "Name") %>%
  semi_join(vg, by = "Platform")

## Add rows removed from validation set back into vg set
removed <- anti_join(temp, validation)
vg <- rbind(vg, removed)

rm(data, temp, test_index, removed)

## View first ten rows of vg data
vg %>% slice(1:10) %>% knitr::kable(caption = "Table 1.1. First ten rows of vg dataset",
                                     row.names = FALSE) %>%
  kable_styling(font_size = 10, position = "center",
                latex_options = c("scale_down","HOLD_position"))
```

In table 1.2, the summary statistics on the vg set reveal almost 12 thousand unique games. This number is less than the total rows because a number of games are released on multiple platforms (*e.g*, Xbox, Playstation).   

```{r vg summary table, message=FALSE, warning=FALSE, echo=FALSE}
## Convert Critic_Score NAs to mean of Critic_Score in vg
vg$Critic_Score[is.na(vg$Critic_Score)] <- mean(vg$Critic_Score, na.rm = T)

## Make User_Score numeric
vg <- transform(vg, User_Score = as.numeric(User_Score))

## Convert User_Score NAs to mean of User_Score in vg
vg$User_Score[is.na(vg$User_Score)] <- mean(vg$User_Score, na.rm = T)

## Create vg summary table
vg_summary <- data.frame(Rows = nrow(vg),
                          Columns = ncol(vg),
                          "Unique games" = n_distinct(vg$Name),
                          "Unique platforms" = n_distinct(vg$Platform),
                          "Average global sales" = round(mean(vg$Global_Sales),2),
                          "Average critic score" = round(mean(vg$Critic_Score),2),
                          "Average user score" = round(mean(vg$User_Score),2),
                          "Number of genres" = n_distinct(vg$Genre),
                          check.names = FALSE)

## Display the vg summary table
vg_summary %>% 
  knitr::kable(caption = "Table 1.2. Summary of vg set") %>%
  kable_styling(font_size = 10, position = "center",
                latex_options = c("scale_down","HOLD_position"))
```

The vg data contains other useful pieces of information for creating a game recommendation algorithm. For example, the year a given game was released. The rating submission dates in the vg set range from 1980 to 2020.

Another example is the genre information for each game. Most games in the vg set have one genre listed. In fact, only two entries the dataset---rated by 0 users/critics---have no genre (nor even a name) listed. These variables, along with the others previously mentioned, will be useful predictors in the model.   

```{r vg no genre listed, message=FALSE, warning=FALSE, echo=FALSE}
## Display rows in vg with no genre information
vg %>%
  filter(Genre == "") %>% 
  knitr::kable(caption = "Table 1.3. Rows in vg lacking genre information",
               row.names = FALSE) %>%
  kable_styling(font_size = 10, position = "center",
                latex_options = c("scale_down","HOLD_position"))
```

# 1.2 Executive summary  

Through data science techniques, we can create a strong prediction model capable of recommending games to individuals. This can be beneficial for providing entertainment suggestions to individuals. An important question is: how can we model this efficiently with a large dataset? Machine learning methods are capable of achieving this, and that is the strategy I pursue in this report.

In the sections that follow I discuss my methods, clean and organize my data, develop my model, and finally evaluate the predictions generated by the model. I find success in predicting game ratings based on five variables present in the dataset. The result is a suitably low measure of difference between the predictions my model generates and the actual rating values in a tranche of the dataset.

# 2.0 Methods and analysis

For this project, my goal is to develop a video game recommendation model. This model will aim to accurately predict game ratings in the final hold-out test set as if they were not known to me. The means of evaluating my success will be minimizing root mean squared error (RMSE). The RMSE is the standard deviation of the residuals, a common measurement used in statistics to evaluate accuracy of projections. This project has a defined target of reaching the lowest possible RMSE value given the available data.  

The key dependent variable of interest for recommending games is the user score. This rating measures how much a user enjoyed a given game. In the vg data, ratings range from a minimum of 0 to a maximum of 9.6. The overall mean value for the game ratings in the dataset is 7.14, and the standard deviation is 0.99. Table 2.1 displays the number of observations (n), mean, standard deviation (sd), minimum and maximum ratings, the range of the rating scale, and the standard error (se).  

```{r ratings summary statistics, message=FALSE, warning=FALSE, echo=FALSE}
## Create and display summary table of user scores
describe(vg$User_Score, fast = TRUE) %>%
  select(-vars) %>%
  knitr::kable(caption = "Table 2.1. Summary statistics for user scores",
               row.names = FALSE) %>%
  kable_styling(font_size = 10, position = "center",
                latex_options = c("HOLD_position"))
```

Further exploring the data reveals additional patterns in user ratings. As shown in figure 2.1, we see that the most common rating is 7.8. Additionally, the data appear to be left-skewed. A left-skewed distribution, as shown by the long left tail in figure 2.1, indicates that ratings concentrate on the right side of the tail, such that reviewers tend to give high ratings to games.

\newpage

\begin{center}
Figure 2.1. Count by user score in vg
\end{center}

```{r scores summed, fig.align = 'center', fig.width = 4, fig.height = 3, message=FALSE, warning=FALSE, echo=FALSE}
## Create a table with the sum of each user score
u_s_sum <- vg %>% filter(User_Score != mean(User_Score)) %>% group_by(User_Score) %>%
  summarize(count = n())

## Plot the count by user score using the user score sum table
u_s_sum %>%
  ggplot(aes(User_Score, count)) +
  geom_col(fill = "steel blue", color = "black") +
  theme_classic() +
  theme(plot.background = element_rect(color = "black", fill=NA, size=0.25)) + 
  labs(x = "User score", y = "Count")
```

As shown in figure 2.2, most games have very few scores. Additionally, there does not appear to be any real trend in the data. Games with few ratings, have scores both low and high, and similarly, games with many ratings have both high and mediocre scores. 

\begin{center}
Figure 2.2. Count by user score in vg
\end{center}

```{r average and count scatter, fig.align = 'center', fig.width = 4, fig.height = 3, message=FALSE, warning=FALSE, echo=FALSE}
## Create and display scatterplot of average and number of user scores
vg %>% 
  ggplot(aes(User_Count, User_Score)) +
  geom_point(color = "steel blue", alpha = 0.3) +
  geom_smooth() +
  theme_classic() +
  theme(plot.background = element_rect(color = "black", fill=NA, size=0.25)) +
  labs(x = "Number of user scores",
       y = "Average score")
```

Accurate prediction of game scores provides justification for recommending games. In brief, if my model predicts a game will receive a score of 1, I would not encourage someone using this recommendation system to play the game. The reverse is true if my model predicts a game will receive a score of 9.  

# 2.1 Techniques and processes

My techniques and processes will include the following:
1. cleaning the data,
2. exploring and visualizing the data using the tidyverse package,
3. using insights gained to develop a model concept,
4. creating the model, and
5. evaluating the model on the validation set by measuring the RMSE.  

## 2.1.1 Data cleaning

First, I update vg, multiplying user score by 10 to scale to the provided critic score. Therefore, the final range of user scores goes from 0 to 100. This first data cleaning effort ensures that the scores are comparable. Table 2.2 displays the first five rows of the updated vg set with scores converted. 

```{r view updated vg top, message=FALSE, warning=FALSE, echo=FALSE}
## Update vg with user score multiplied by 10 to scale to critic score
vg <- vg %>% 
  mutate(User_Score = User_Score*10)

## View first five rows of updated vg dataset
vg %>% slice(1:5) %>% knitr::kable(caption = "Table 2.2. First five rows of updated vg dataset",
                                    row.names = FALSE) %>%
  kable_styling(font_size = 10, position = "center",
                latex_options = c("scale_down","HOLD_position"))
```

## 2.1.2 Data exploration and visualization

As demonstrated in table 1.2, the vg set contains 11,563 unique games. Summary statistics on ratings by critic are shown in table 2.3. The average critic score is 68.9, with a standard deviation of 9.6. Critics on average rated movies slightly lower than users, but with somewhat less spread.

```{r summary of ratings by game, message=FALSE, warning=FALSE, echo=FALSE}
## Create and display summary table of critic scores
describe(vg$Critic_Score, fast = TRUE) %>%
  select(-vars) %>%
  knitr::kable(caption = "Table 2.3. Summary statistics for critic scores",
               row.names = FALSE) %>%
  kable_styling(font_size = 10, position = "center",
                latex_options = c("HOLD_position"))
```

Figure 2.3 shows the distribution of critic ratings per game. Similarly to user ratings, the ratings are left-skewed. This shows that, much like users, critics tend to give higher ratings to games.

\begin{center}
Figure 2.3. Distribution of critic scores
\end{center}

```{r distribution of ratings per game, fig.align = 'center', fig.width = 4, fig.height = 3, message=FALSE, warning=FALSE, echo=FALSE}
## Create a table with the sum of each critic score
c_s_sum <- vg %>% filter(Critic_Score != mean(Critic_Score)) %>% group_by(Critic_Score) %>%
  summarize(count = n())

## Plot the count by critic score using the critic score sum table
c_s_sum %>%
  ggplot(aes(Critic_Score, count)) +
  geom_col(fill = "steel blue", color = "black") +
  theme_classic() +
  theme(plot.background = element_rect(color = "black", fill=NA, size=0.25)) + 
  labs(x = "Critic score", y = "Count")
```

There appears to be a correlation between the number of scores that a game receives from critics, and the average score received from critics (figure 2.4). Games that garner more attention from critics are perhaps more unique, interesting, or simply better games; if so, it is not surprising that there would be a positive relationship between the number of critic scores a game receives and the average score.

\begin{center}
Figure 2.4. Average game ratings by critics
\end{center}

```{r average ratings by number of ratings, fig.align = 'center', fig.width = 4, fig.height = 3, message=FALSE, warning=FALSE, echo=FALSE}
## Create and display scatterplot of average and number of critic scores
vg %>% 
  ggplot(aes(Critic_Count, Critic_Score)) +
  geom_point(color = "steel blue", alpha = 0.3) +
  geom_smooth() +
  theme_classic() +
  theme(plot.background = element_rect(color = "black", fill=NA, size=0.25)) +
  labs(x = "Number of critic scores",
       y = "Average score")
```

We can also investigate the number of games by release year. As we see in figure 2.5, more games were released in the late 2000s. Very few games in the dataset were released in the 1980s, slowly increasing in the mid 1990s. The number of games released in a single year peaked in the years 2008 and 2009. The number of games released by year declined at that point, and was fairly stable from 2012 to 2016. 

\begin{center}
Figure 2.5. Count of game releases by year
\end{center}

```{r average and overall ratings distributions, fig.align = 'center', fig.height = 3, message=FALSE, warning=FALSE, echo=FALSE}
## create quick summary table for plot of the number of user scores by year of release
year_sum <- vg %>% filter(Year_of_Release != "N/A") %>% group_by(Year_of_Release) %>%
  summarize(n = n(), Average_user_score = mean(User_Score))

## Display plot of number of user scores by year of release
ggplot(year_sum, aes(Year_of_Release, n)) +
  geom_col(fill = "steel blue", color = "black") +
  theme_classic() +
  labs(x = "Year",
       y = "Count") + 
  theme(axis.text.x=element_text(angle = 80, hjust = 1),
        plot.background = element_rect(color = "black", fill=NA, size=0.25))
```

The average rating by year of game release appears to have a time trend. Ratings held relatively constant from the 1980s to the early 1990s, peaked in the 2000s, and have declined since. Average ratings by year range from the high 60s to the mid 70s out of 100. There are few outliers in the data.  


\begin{center}
Figure 2.5. Average ratings per year
\end{center}

```{r average ratings per year, fig.align = 'center', fig.width = 4, fig.height = 2.5, message=FALSE, warning=FALSE, echo=FALSE}
## Plot the average user score by year of release
ggplot(year_sum, aes(as.numeric(Year_of_Release), Average_user_score)) +
  geom_point(color = "steel blue", alpha = 0.6) +
  geom_smooth(color = "steel blue") +
  theme_classic() +
  labs(x = "Year",
       y = "Rating") + 
  theme(plot.background = element_rect(color = "black", fill=NA, size=0.25))
```

A simple linear regression model provides more information on the relationship between release year and average rating. Two models are estimated and results reported in table 2.4. Model 1 estimates the effect of the release year and squared release year on average rating; the squared term is included to account for the non-linearity of average ratings by release year illustrated in figure 2.9. In model 2, a cubic term is added for comparison because the curve of average ratings by release year appears to have a point of inflection. The significant coefficients in both models indicate that release year impacts average rating, and that the impact varies over time.  

\newpage

\begin{center}
Table 2.4. Linear models of average user score by game release year
\end{center}

```{r basic linear model results, results='asis', message=FALSE, warning=FALSE, echo=FALSE}
## Fit linear model of average score on year of release and year of release squared
fit_lm1 <- lm(Average_user_score ~ I(as.numeric(Year_of_Release)^2) +
                I(as.numeric(Year_of_Release)), 
              data = year_sum)

## Fit linear model of average rating on year of release, year of release squared, and year of release cubed
fit_lm2 <- lm(Average_user_score ~ I(as.numeric(Year_of_Release)^3) +
                I(as.numeric(Year_of_Release)^2) +
                I(as.numeric(Year_of_Release)), 
              data = year_sum)

## Create table of results for both linear models
export_summs(fit_lm1, fit_lm2)
```

\begin{center}
Note: standard errors in parentheses.
\end{center}

Games in the vg data are categorized into various genres. The distinct genres included in the dataset are listed in table 2.5. There are 12 unique genres (as well as the aforementioned "no genres listed" indicator, omitted from the table). 

```{r vg genre statistics, message=FALSE, warning=FALSE, echo=FALSE}
## Create and display a table describing the genres in vg
genres_sum <- vg %>% filter(Genre != "") %>% group_by(Genre) %>%
  summarize(Count = n(),
            User_mean = mean(User_Score),
            Critic_mean = mean(Critic_Score))
genres_sum %>%
  knitr::kable(caption = "Table 2.5. Statistics by genres in vg",
             digits = 2,
             row.names = FALSE) %>%
  kable_styling(font_size = 10, position = "center",
                latex_options = c("HOLD_position"))
```


Role-playing games are the highest rated on average among the vg genres. Figure 2.6 shows they rate above 70 on average. Conversely, the lowest rated games on average belong to the sports genre. The figure provides evidence that ratings vary by genre, though minimally.  

\newpage

\begin{center}
Figure 2.6. Average user score by genre
\end{center}

```{r average rating by genre, fig.align = 'center', fig.width = 4, fig.height = 3, message=FALSE, warning=FALSE, echo=FALSE}
## Plot average user score by genre
genres_sum %>%
  ggplot(
    aes(x = reorder(Genre, User_mean), User_mean)) +
  geom_col(fill = "steel blue", color = "black") +
  theme_classic() +
  coord_flip() +
  labs(
    y = "Average user score",
    x = "Genres") + 
  theme(plot.background = element_rect(color = "black", fill=NA, size=0.25))
```

Do genre effects interact with user effects? To investigate, we can take a random sample of 10 games from vg and explore how they are rated on average across genres. In figure 2.7 we see the average ratings by genre for our random sample. The average ratings and the ranking of genres differs in figure 2.7 compared to figure 2.6. Interestingly, the highest rated genre in figure 2.7, sports, was the lowest rated genre on average in figure 2.6. This suggests that, although there may be overall favorite genres for gamers, certain users have distinct genre preferences. These genre preferences will help inform our final model.  

\begin{center}
Figure 2.7. Average score by genre, random sample of users
\end{center}

```{r average rating by genre for a random sample, fig.align = 'center', fig.width = 4, fig.height = 3, message=FALSE, warning=FALSE, echo=FALSE}

## Create list of games close to the average
game_list <- vg %>% 
  filter(User_Score != mean(User_Score),
         User_Score >= round(mean(User_Score),2)-1,
         User_Score <= round(mean(User_Score),2)+1) %>% 
  select(Name, Genre, User_Score)

## Sample 10 games at random
set.seed(1, sample.kind = "Rounding")
game_list <- sample(game_list$Name, 10)

## Create average user score figure by genre for the 10 random games
vg_random <- vg %>%
  filter(Name %in% game_list)

random_genres <- vg_random %>% group_by(Genre) %>%
  summarize(Count = n(),
            User_mean = mean(User_Score),
            Critic_mean = mean(Critic_Score))

## Plot average user score by genre for random sample of 10 users
random_genres %>%
  ggplot(
    aes(x = reorder(Genre, User_mean), User_mean)) +
  geom_col(fill = "steel blue", color = "black") +
  theme_classic() +
  coord_flip() +
  labs(
    y = "Average user score",
    x = "Genres") + 
  theme(plot.background = element_rect(color = "black", fill=NA, size=0.25))
```

## 2.1.3 Insights gained

The exercise of data exploration and visualization resulted in several useful insights. Ratings data are unbalanced---a large number of games have very few ratings. Substantial publisher to publisher variation is present in the vg data. Certain publishers release games that are rated higher than their counterparts. The ratings of games are correlated with their release years. Finally, genres matter for game video game ratings overall overall.  

# 2.2 Modeling approach

My approach to developing the model is to begin with a naive attempt using a simple average. From there, I will tune my model, adding predictors incrementally in an effort to minimize RMSE. I plan to use predictors based on the insights detailed above, to ultimately produce a model that accounts for various unique characteristics of each game.

Values of the key dependent variable, ratings, in vg are discrete. In other words ratings values are measured by counting. One improvement in my model is that it will allow for ratings predictions that are continuous. The result should be a more sensitive measure and, by extension, a lower RMSE.

# 3.0 Results

In this section I develop my model to minimize the RMSE of predicted game ratings and actual ratings in the vg dataset, then present the results of my model. As previously described, I adopt an iterative approach, adding predictors to the model one-by-one.  

# 3.1 Model results

The most straightforward model for a video game recommendation system would simply predict the same rating for each game, not taking into account unique characteristics of the game, its sales numbers, or the year the game was released. Such a model would use the average to minimize the RMSE. With the vg data, a model using only a simple average of game ratings results in a RMSE of 9.9250, as shown in table 3.1.  

```{r naive model, message=FALSE, warning=FALSE, echo=FALSE}
## Compute RMSE using the average of all user scores
vg <- vg %>% mutate(mu_user = mean(User_Score))

rmses <- data_frame(method = "Only estimate is the average",
                    RMSE = RMSE(vg$mu_user, vg$User_Score))

## Create the RMSE results table
rmses %>%
  knitr::kable(caption = "Table 3.1. RMSE by method I",
               row.names = FALSE,
               digits = 4) %>%
  kable_styling(font_size = 10, position = "center",
                latex_options = "HOLD_position")
```

The RMSE of 9.9250 is somewhat high---it would translate to regularly missing ratings predictions by nearly 10 percentage points. We can refer back to table 2.1 to confirm that our current model RMSE is equal to the standard deviation of user scores in vg. A reasonable goal is to improve upon this RMSE. In an effort to do so, we can tweak the model so that instead of using a simple average as the estimate, we incorporate the critic score average. Table 3.2 adds a new row to report the RMSE for our critic model.  

```{r critic-specific model, message=FALSE, warning=FALSE, echo=FALSE}
## Add critic--average effect to vg
b_c_sum <- vg %>% mutate(yhat = User_Score - mu_user) %>%
  group_by(Critic_Score) %>%
  summarize(b_c = mean(yhat))

vg <- left_join(vg, b_c_sum, by = "Critic_Score") %>%
  mutate(mu_c = mu_user + b_c)

## Append the row to the RMSE results table
rmses <- bind_rows(rmses,
                   data_frame(method = "Add critic–specific effect",
                              RMSE = RMSE(vg$mu_c, vg$User_Score)))

## Display the RMSE results table
rmses %>%
  knitr::kable(caption = "Table 3.2. RMSE by method II",
               row.names = FALSE,
               digits = 4) %>%
  kable_styling(font_size = 10, position = "center",
                latex_options = "HOLD_position")
```

We see that our updated model has improved RMSE, lowering it to 8.4753. The next step is to account for platform-specific effects. Table 3.3 adds another new row to show the results of the model incorporating an estimate for the platform effect.  

```{r platform model, message=FALSE, warning=FALSE, echo=FALSE}
## Add platform effect to vg
b_p_sum <- vg %>% mutate(error = User_Score - mu_c) %>%
  group_by(Platform) %>%
  summarize(b_p = mean(error))

vg <- left_join(vg, b_p_sum, by = "Platform") %>%
  mutate(mu_c_p = mu_c + b_p)

## Append the row to the RMSE results table
rmses <- bind_rows(rmses,
                   data_frame(method = "Add platform effect",
                              RMSE = RMSE(vg$mu_c_p, vg$User_Score)))

## Display the RMSE results table
rmses %>%
  knitr::kable(caption = "Table 3.3. RMSE by method III",
               row.names = FALSE,
               digits = 4) %>%
  kable_styling(font_size = 10, position = "center",
                latex_options = "HOLD_position")
```

Incorporating an estimate for the effect of platforms improves our model performance once again. Now we need to address the variation of ratings over time in our data. We will add a time effect using the variable of a game's release year. The resulting model RMSE is provided in table 3.4.  

```{r time model, message=FALSE, warning=FALSE, echo=FALSE}
## Add year effect to vg
b_y_sum <- vg %>% mutate(error = mu_c - mu_c_p) %>%
  group_by(Year_of_Release) %>%
  summarize(b_y = mean(error))

vg <- left_join(vg, b_y_sum, by = "Year_of_Release") %>%
  mutate(mu_c_p_y = mu_c_p + b_y)

## Append the row to the RMSE results table
rmses <- bind_rows(rmses,
                   data_frame(method = "Add year effect",
                              RMSE = RMSE(vg$mu_c_p_y, vg$User_Score)))

## Display the RMSE results table
rmses %>%
  knitr::kable(caption = "Table 3.4. RMSE by method IV",
               row.names = FALSE,
               digits = 4) %>%
  kable_styling(font_size = 10, position = "center",
                latex_options = "HOLD_position")
```

The measure of release year interestingly represents an setback to the model, increasing RMSE from 8.1933 to 8.4003. The logical final step to round out the model is adding a treatment for game sales. The result of incorporating sales effects into the model is depicted in Table 3.5.  

```{r sales model, message=FALSE, warning=FALSE, echo=FALSE}
## Add sales effect to vg
b_s_sum <- vg %>% mutate(error = User_Score - mu_c_p_y) %>%
  group_by(Global_Sales) %>%
  summarize(b_s = mean(error))

vg <- left_join(vg, b_s_sum, by = "Global_Sales") %>%
  mutate(mu_c_p_y_s = mu_c_p_y + b_s)

## Append the row to the RMSE results table
rmses <- bind_rows(rmses,
                   data_frame(method = "Add sales effect",
                              RMSE = RMSE(vg$mu_c_p_y_s, vg$User_Score)))

## Display the RMSE results table
rmses %>%
  knitr::kable(caption = "Table 3.5. RMSE by method V",
               row.names = FALSE,
               digits = 4) %>%
  kable_styling(font_size = 10, position = "center",
                latex_options = "HOLD_position")
```

Finally, we model the effect of publishers on user ratings. Certain publishers could have a knack for making more popular, better-received games. Table 3.6 reveals these results and the RMSE decrease associated with added publisher controls.

```{r publisher model, message=FALSE, warning=FALSE, echo=FALSE}
## Add publisher effect to vg
b_pub_sum <- vg %>% mutate(error = User_Score - mu_c_p_y_s) %>%
  group_by(Publisher) %>%
  summarize(b_pub = mean(error))

vg <- left_join(vg, b_pub_sum, by = "Publisher") %>%
  mutate(mu_c_p_y_s_pub = mu_c_p_y_s + b_pub)

## Append the row to the RMSE results table
rmses <- bind_rows(rmses,
                   data_frame(method = "Add publisher--effect",
                              RMSE = RMSE(vg$mu_c_p_y_s_pub, vg$User_Score)))

## Display the RMSE results table
rmses %>%
  knitr::kable(caption = "Table 3.6. RMSE by method VI",
               row.names = FALSE,
               digits = 4) %>%
  kable_styling(font_size = 10, position = "center",
                latex_options = "HOLD_position")
```

With all variables included, our model results in an RMSE of 7.9849 on the vg ratings data. The culmination of the project is to evaluate the model's performance on the final hold-out test set.  

# 3.2 Model performance

The first step of evaluating the final model on the validation dataset is to repeat our data cleaning procedure in order to get the measure of user ratings into a numerical value and scaled with critical ratings. Then, we will append columns for the final model effects---critic, platform, sales, and publisher. Finally, we will calculate RMSE, measuring success by minimizing its value.  

Before performing the final evaluation, we need to address not applicable (NA) values in the variable predictors appended to the validation set. Table 3.7 lists these. To deal with them, I convert each NA to the average value of the corresponding effect in the validation set.  

```{r final model preparation, message=FALSE, warning=FALSE, echo=FALSE}
## Make User_Score numeric in validation
validation <- transform(validation, User_Score = as.numeric(User_Score))

## Convert User_Score NAs to mean of User_Score in validation
validation$User_Score[is.na(validation$User_Score)] <- mean(validation$User_Score, na.rm = T)

## Update vg with user score multiplied by 10 to scale to critic score
validation <- validation %>% 
  mutate(User_Score = User_Score*10)

## Add mu_user to validation
validation <- validation %>% mutate(mu_user = rep(vg$mu_user[1],times = nrow(validation)))

## Append effects to validation
validation <- validation %>%
  left_join(b_c_sum, by = "Critic_Score") %>%
  left_join(b_p_sum, by = "Platform") %>%
  left_join(b_y_sum, by = "Year_of_Release") %>%
  left_join(b_s_sum, by = "Global_Sales") %>%
  left_join(b_pub_sum, by = "Publisher")

## Check for NAs in validation set by displaying a table
knitr::kable(data.frame(NAs = colSums(is.na(validation[,17:21]))),
             caption = "Table 3.7. Check for NA values in validation set") %>% 
  kable_styling(font_size = 10, position = "center",
                latex_options = "HOLD_position")
```

With the NA values converted, we can evaluate the final model. Results are shown in table 3.8.  

```{r final model evaluation, message=FALSE, warning=FALSE, echo=FALSE}
## Convert NAs to mean of corresponding effects in validation
validation$b_c[is.na(validation$b_c)] <- mean(validation$b_c, na.rm = T)
validation$b_s[is.na(validation$b_s)] <- mean(validation$b_s, na.rm = T)
validation$b_pub[is.na(validation$b_pub)] <- mean(validation$b_pub, na.rm = T)

## Combine effects for final predicted ratings
validation <- validation %>%
  mutate(predicted_rating = mu_user + b_c + b_p + b_s + b_pub)

## Create and display the final RMSE table
data_frame(method = "Critic-average, platform, sales, publisher model",
           RMSE = RMSE(validation$User_Score, validation$predicted_rating)) %>%
  knitr::kable(caption = "Table 3.8. Final RMSE evaluation",
               row.names = FALSE,
               digits = 4) %>%
  kable_styling(font_size = 10, position = "center",
                latex_options = "HOLD_position")
```

My model's final RMSE is 11.0988. The effects included in the model allow us to make predictions that are reasonably close to a given user score out of 100.

# 4.0 Conclusion

This project involved making predictions on video game user scores using machine learning techniques. The contribution of this work is demonstrating how simple methods can result in strong predictive power on variables in large datasets. Individuals who want to entertain themselves with video games could find this study of interest. But there are many other potential practical applications for the techniques used in this analysis. I will discuss those after summarizing the report and addressing the limitations of this work.  

# 4.1 Report summary

With the aim of developing an accurate video game recommendation model, I performed analysis on a large dataset. The data were thousands of video game ratings with corresponding information on number of users giving ratings, critical reception, release time, genre, sales figures, and publishers. My efforts involved summarizing the data, cleaning the data, performing exploratory analysis and data visualization. The result was added insight which, in turn, I put to use while considering strategies for modeling video game ratings. I developed my model, electing for an iterative process to drive RMSE lower in increments. Adding in effects by critic score, genre, platform, publisher, and sales allowed my model to account for the rich variability observed across all. Ultimately, I was able to achieve the goal of an appropriately low RMSE when evaluating my model's predictions compared to the ratings in the final hold-out test set.  

# 4.2 Limitations

My project, while successful, is not without drawbacks. In large part my analysis was confounded by many missing user-rating values. The same was true for ratings by critics. Genre information for the video games present in the dataset was lacking, and future data collection should prioritize listing multiple genres per game. Additionally, a crucial missing data component was unique user identifiers---being able to disentangle user-specific individual effects would likely have improved my model considerably.

Aside from that, my analysis uncovered some odd patterns in the data---for example, average game ratings increasing substantially once roughly 600 weeks have passed since the first rating. This is an example of a phenomenon that is difficult to explain, and perhaps inadequately addressed in my analysis. Another possible limitation of this work involves the handling of NA values in the vg and validation sets. I opted to convert those to the column average because it was a simple choice. But it is possible that closer investigation would uncover a better way to deal with the NAs. Regardless, the large number of NAs, particularly for the key dependent variable of user scores, was a detriment to my study.  

# 4.3 Future work

Future analysis would benefit from improved data. Even beyond what I mentioned in the previous section, there are likely more than 12 unique genres that are informative with respect to video games. In addition, it is likely that new or more advanced modeling techniques could push the RMSE down further. I am satisfied with the predictive power of my recommendation algorithm, but there is likely room for improvement in future studies.

It is easy to imagine applications of this work to areas outside of video recommendations. For example, a game recommendation system would dovetail nicely with this analysis. Other natural extension of this work could apply to shopping or eating at restaurants. Models such as mine could play a role in better understanding how people enjoy leisure time; this has both lifestyle and policy implications. Future studies should focus on these important concepts.